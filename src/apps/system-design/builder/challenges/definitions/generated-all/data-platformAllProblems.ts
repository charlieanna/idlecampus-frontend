import { ProblemDefinition } from '../../../types/problemDefinition';
import { validConnectionFlowValidator } from '../../../validation/validators/commonValidators';
import { basicFunctionalValidator } from '../../../validation/validators/featureValidators';
import { generateScenarios } from '../../scenarioGenerator';
import { problemConfigs } from '../../problemConfigs';

/**
 * Data-platform Problems - Complete Set
 * Auto-generated from ALL_PROBLEMS.md
 * Total: 18 problems
 */

/**
 * Uber Real-Time Data Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatformUberProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-uber',
  title: 'Uber Real-Time Data Platform',
  description: `Design Uber's data platform ingesting trip, payment, and driver data in real-time, supporting both streaming analytics and ML feature computation.
- Ingest 1 trillion events daily
- Support sub-minute data freshness
- Enable SQL queries on streaming data
- Compute ML features in real-time`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Ingest 1 trillion events daily',
    'Support sub-minute data freshness',
    'Enable SQL queries on streaming data',
    'Compute ML features in real-time',
    'Support time-travel queries'
  ],
  userFacingNFRs: [
    'Latency: P99 < 1 minute end-to-end',
    'Dataset Size: 1 exabyte total capacity',
    'Availability: 99.95% data availability'
  ],

  functionalRequirements: {
    mustHave: [

    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-uber', problemConfigs['l5-data-platform-uber'], [
    'Ingest 1 trillion events daily',
    'Support sub-minute data freshness',
    'Enable SQL queries on streaming data',
    'Compute ML features in real-time',
    'Support time-travel queries'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
events = {}
real = {}

def ingest_1_trillion_events_daily(**kwargs) -> Dict:
    """
    FR-1: Ingest 1 trillion events daily
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_sub_minute_data_freshness(**kwargs) -> Dict:
    """
    FR-2: Support sub-minute data freshness
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_sql_queries_on_streaming_data(**kwargs) -> Dict:
    """
    FR-3: Enable SQL queries on streaming data
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def compute_ml_features_in_real_time(**kwargs) -> Dict:
    """
    FR-4: Compute ML features in real-time
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_time_travel_queries(**kwargs) -> Dict:
    """
    FR-5: Support time-travel queries
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}`,
};

/**
 * Databricks Data Lake Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform1ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-1',
  title: 'Databricks Data Lake Platform',
  description: `Databricks needs to implement data lake to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data lake at Databricks scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data lake at Databricks scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 10M requests per second',
    'Dataset Size: 100TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-1', problemConfigs['l5-data-platform-1'], [
    'Support data lake at Databricks scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_lake_at_databricks_scale(**kwargs) -> Dict:
    """
    FR-1: Support data lake at Databricks scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Snowflake Etl Pipeline Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform2ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-2',
  title: 'Snowflake Etl Pipeline Platform',
  description: `Snowflake needs to implement ETL pipeline to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ETL pipeline at Snowflake scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ETL pipeline at Snowflake scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 11M requests per second',
    'Dataset Size: 110TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-2', problemConfigs['l5-data-platform-2'], [
    'Support ETL pipeline at Snowflake scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_etl_pipeline_at_snowflake_scale(**kwargs) -> Dict:
    """
    FR-1: Support ETL pipeline at Snowflake scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Palantir Real-Time Analytics Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform3ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-3',
  title: 'Palantir Real-Time Analytics Platform',
  description: `Palantir needs to implement real-time analytics to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support real-time analytics at Palantir scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support real-time analytics at Palantir scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 12M requests per second',
    'Dataset Size: 120TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-3', problemConfigs['l5-data-platform-3'], [
    'Support real-time analytics at Palantir scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
events = {}
memory = {}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-1: Support real-time analytics at Palantir scale
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Splunk Ml Feature Store Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform4ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-4',
  title: 'Splunk Ml Feature Store Platform',
  description: `Splunk needs to implement ML feature store to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ML feature store at Splunk scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ML feature store at Splunk scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 13M requests per second',
    'Dataset Size: 130TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-4', problemConfigs['l5-data-platform-4'], [
    'Support ML feature store at Splunk scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def create_item(item_id: str, **kwargs) -> Dict:
    """
    FR-1: Support ML feature store at Splunk scale
    Naive implementation - stores item in memory
    """
    items[item_id] = {
        'id': item_id,
        'created_at': datetime.now(),
        **kwargs
    }
    return items[item_id]

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Tableau Data Governance Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform5ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-5',
  title: 'Tableau Data Governance Platform',
  description: `Tableau needs to implement data governance to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data governance at Tableau scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data governance at Tableau scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 14M requests per second',
    'Dataset Size: 140TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-5', problemConfigs['l5-data-platform-5'], [
    'Support data governance at Tableau scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_governance_at_tableau_scale(**kwargs) -> Dict:
    """
    FR-1: Support data governance at Tableau scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Looker Data Lake Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform6ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-6',
  title: 'Looker Data Lake Platform',
  description: `Looker needs to implement data lake to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data lake at Looker scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data lake at Looker scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 15M requests per second',
    'Dataset Size: 150TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-6', problemConfigs['l5-data-platform-6'], [
    'Support data lake at Looker scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_lake_at_looker_scale(**kwargs) -> Dict:
    """
    FR-1: Support data lake at Looker scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Domo Etl Pipeline Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform7ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-7',
  title: 'Domo Etl Pipeline Platform',
  description: `Domo needs to implement ETL pipeline to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ETL pipeline at Domo scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ETL pipeline at Domo scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 16M requests per second',
    'Dataset Size: 160TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-7', problemConfigs['l5-data-platform-7'], [
    'Support ETL pipeline at Domo scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_etl_pipeline_at_domo_scale(**kwargs) -> Dict:
    """
    FR-1: Support ETL pipeline at Domo scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Sisense Real-Time Analytics Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform8ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-8',
  title: 'Sisense Real-Time Analytics Platform',
  description: `Sisense needs to implement real-time analytics to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support real-time analytics at Sisense scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support real-time analytics at Sisense scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 17M requests per second',
    'Dataset Size: 170TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-8', problemConfigs['l5-data-platform-8'], [
    'Support real-time analytics at Sisense scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
events = {}
memory = {}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-1: Support real-time analytics at Sisense scale
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * ThoughtSpot Ml Feature Store Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform9ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-9',
  title: 'ThoughtSpot Ml Feature Store Platform',
  description: `ThoughtSpot needs to implement ML feature store to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ML feature store at ThoughtSpot scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ML feature store at ThoughtSpot scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 18M requests per second',
    'Dataset Size: 180TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-9', problemConfigs['l5-data-platform-9'], [
    'Support ML feature store at ThoughtSpot scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def create_item(item_id: str, **kwargs) -> Dict:
    """
    FR-1: Support ML feature store at ThoughtSpot scale
    Naive implementation - stores item in memory
    """
    items[item_id] = {
        'id': item_id,
        'created_at': datetime.now(),
        **kwargs
    }
    return items[item_id]

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Alteryx Data Governance Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform10ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-10',
  title: 'Alteryx Data Governance Platform',
  description: `Alteryx needs to implement data governance to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data governance at Alteryx scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data governance at Alteryx scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 19M requests per second',
    'Dataset Size: 190TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-10', problemConfigs['l5-data-platform-10'], [
    'Support data governance at Alteryx scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_governance_at_alteryx_scale(**kwargs) -> Dict:
    """
    FR-1: Support data governance at Alteryx scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Informatica Data Lake Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform11ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-11',
  title: 'Informatica Data Lake Platform',
  description: `Informatica needs to implement data lake to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data lake at Informatica scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data lake at Informatica scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 20M requests per second',
    'Dataset Size: 200TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-11', problemConfigs['l5-data-platform-11'], [
    'Support data lake at Informatica scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_lake_at_informatica_scale(**kwargs) -> Dict:
    """
    FR-1: Support data lake at Informatica scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Talend Etl Pipeline Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform12ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-12',
  title: 'Talend Etl Pipeline Platform',
  description: `Talend needs to implement ETL pipeline to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ETL pipeline at Talend scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ETL pipeline at Talend scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 21M requests per second',
    'Dataset Size: 210TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-12', problemConfigs['l5-data-platform-12'], [
    'Support ETL pipeline at Talend scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_etl_pipeline_at_talend_scale(**kwargs) -> Dict:
    """
    FR-1: Support ETL pipeline at Talend scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Fivetran Real-Time Analytics Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform13ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-13',
  title: 'Fivetran Real-Time Analytics Platform',
  description: `Fivetran needs to implement real-time analytics to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support real-time analytics at Fivetran scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support real-time analytics at Fivetran scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 22M requests per second',
    'Dataset Size: 220TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-13', problemConfigs['l5-data-platform-13'], [
    'Support real-time analytics at Fivetran scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
events = {}
memory = {}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-1: Support real-time analytics at Fivetran scale
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Stitch Ml Feature Store Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform14ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-14',
  title: 'Stitch Ml Feature Store Platform',
  description: `Stitch needs to implement ML feature store to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ML feature store at Stitch scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ML feature store at Stitch scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 23M requests per second',
    'Dataset Size: 230TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-14', problemConfigs['l5-data-platform-14'], [
    'Support ML feature store at Stitch scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def create_item(item_id: str, **kwargs) -> Dict:
    """
    FR-1: Support ML feature store at Stitch scale
    Naive implementation - stores item in memory
    """
    items[item_id] = {
        'id': item_id,
        'created_at': datetime.now(),
        **kwargs
    }
    return items[item_id]

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Segment Data Governance Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform15ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-15',
  title: 'Segment Data Governance Platform',
  description: `Segment needs to implement data governance to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data governance at Segment scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data governance at Segment scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 24M requests per second',
    'Dataset Size: 240TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-15', problemConfigs['l5-data-platform-15'], [
    'Support data governance at Segment scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_governance_at_segment_scale(**kwargs) -> Dict:
    """
    FR-1: Support data governance at Segment scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Census Data Lake Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform16ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-16',
  title: 'Census Data Lake Platform',
  description: `Census needs to implement data lake to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support data lake at Census scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support data lake at Census scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 25M requests per second',
    'Dataset Size: 250TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-16', problemConfigs['l5-data-platform-16'], [
    'Support data lake at Census scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_data_lake_at_census_scale(**kwargs) -> Dict:
    """
    FR-1: Support data lake at Census scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

/**
 * Hightouch Etl Pipeline Platform
 * From extracted-problems/system-design/data-platform.md
 */
export const l5DataPlatform17ProblemDefinition: ProblemDefinition = {
  id: 'l5-data-platform-17',
  title: 'Hightouch Etl Pipeline Platform',
  description: `Hightouch needs to implement ETL pipeline to support their growing infrastructure. The system must handle millions of users while maintaining high availability and supporting hundreds of engineering teams.
- Support ETL pipeline at Hightouch scale
- Enable gradual migration with zero downtime
- Maintain backward compatibility
- Support A/B testing and gradual rollout`,

  // User-facing requirements (interview-style)
  userFacingFRs: [
    'Support ETL pipeline at Hightouch scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ],
  userFacingNFRs: [
    'Latency: P99 < 100ms for all operations',
    'Request Rate: 26M requests per second',
    'Dataset Size: 260TB data migration',
    'Availability: 99.99% uptime during migration'
  ],

  functionalRequirements: {
    mustHave: [
      {
        type: 'load_balancer',
        reason: 'Need LB for high availability and traffic distribution',
      }
    ],
    mustConnect: [

    ],
    dataModel: {
      entities: ['data'],
      fields: {
        data: ['id', 'value', 'created_at'],
      },
      accessPatterns: [
        { type: 'read_by_key', frequency: 'very_high' },
        { type: 'write', frequency: 'medium' },
      ],
    },
  },

  scenarios: generateScenarios('l5-data-platform-17', problemConfigs['l5-data-platform-17'], [
    'Support ETL pipeline at Hightouch scale',
    'Enable gradual migration with zero downtime',
    'Maintain backward compatibility',
    'Support A/B testing and gradual rollout',
    'Provide comprehensive monitoring and rollback'
  ]),

  validators: [
    // Feature-specific validators for each FR
    { name: 'Basic Functionality', validate: basicFunctionalValidator },
    // Generic validators
    {
      name: 'Valid Connection Flow',
      validate: validConnectionFlowValidator,
    },
  ],

  pythonTemplate: `from datetime import datetime
from typing import List, Dict, Optional, Any

# In-memory storage (naive implementation)
data = {}
items = {}
events = {}
memory = {}

def support_etl_pipeline_at_hightouch_scale(**kwargs) -> Dict:
    """
    FR-1: Support ETL pipeline at Hightouch scale
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def enable_gradual_migration_with_zero_downt(**kwargs) -> Dict:
    """
    FR-2: Enable gradual migration with zero downtime
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def maintain_backward_compatibility(**kwargs) -> Dict:
    """
    FR-3: Maintain backward compatibility
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def support_a_b_testing_and_gradual_rollout(**kwargs) -> Dict:
    """
    FR-4: Support A/B testing and gradual rollout
    Naive implementation - placeholder function
    """
    return {'status': 'success', 'data': kwargs}

def track_event(event_type: str, item_id: str, metadata: Dict = None) -> Dict:
    """
    FR-5: Provide comprehensive monitoring and rollback
    Naive implementation - stores event in memory
    """
    event_id = f"{event_type}_{item_id}_{datetime.now().timestamp()}"
    events[event_id] = {
        'id': event_id,
        'type': event_type,
        'item_id': item_id,
        'metadata': metadata or {},
        'created_at': datetime.now()
    }
    return events[event_id]`,
};

